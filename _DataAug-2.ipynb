{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vFyVtErpNMqL"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import tensorflow as tf\n","import cv2\n","from keras import backend as K\n","from keras.layers import Layer,InputSpec\n","import keras.layers as kl\n","from glob import glob\n","from sklearn.metrics import roc_curve, auc\n","from keras.preprocessing import image\n","from tensorflow.keras.models import Sequential\n","from sklearn.metrics import roc_auc_score\n","from tensorflow.keras import callbacks \n","from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n","from  matplotlib import pyplot as plt\n","from tensorflow.keras import Model\n","from tensorflow.keras.layers import concatenate,Dense, Conv2D, MaxPooling2D, Flatten,Input,Activation,add,AveragePooling2D,BatchNormalization,Dropout\n","%matplotlib inline\n","import shutil\n","from sklearn.metrics import  precision_score, recall_score, accuracy_score,classification_report ,confusion_matrix\n","from tensorflow.python.platform import build_info as tf_build_info\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ytjj1YTNMqP"},"outputs":[],"source":["\n","from PIL import ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gJoUlgjnNMqP","outputId":"1f03d0a9-d318-45e7-9b32-1e1d94120f0d"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>lesion_id</th>\n","      <th>image_id</th>\n","      <th>dx</th>\n","      <th>dx_type</th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>localization</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>HAM_0000118</td>\n","      <td>ISIC_0027419</td>\n","      <td>bkl</td>\n","      <td>histo</td>\n","      <td>80.0</td>\n","      <td>male</td>\n","      <td>scalp</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>HAM_0000118</td>\n","      <td>ISIC_0025030</td>\n","      <td>bkl</td>\n","      <td>histo</td>\n","      <td>80.0</td>\n","      <td>male</td>\n","      <td>scalp</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>HAM_0002730</td>\n","      <td>ISIC_0026769</td>\n","      <td>bkl</td>\n","      <td>histo</td>\n","      <td>80.0</td>\n","      <td>male</td>\n","      <td>scalp</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>HAM_0002730</td>\n","      <td>ISIC_0025661</td>\n","      <td>bkl</td>\n","      <td>histo</td>\n","      <td>80.0</td>\n","      <td>male</td>\n","      <td>scalp</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>HAM_0001466</td>\n","      <td>ISIC_0031633</td>\n","      <td>bkl</td>\n","      <td>histo</td>\n","      <td>75.0</td>\n","      <td>male</td>\n","      <td>ear</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     lesion_id      image_id   dx dx_type   age   sex localization\n","0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp\n","1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp\n","2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp\n","3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp\n","4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["data_pd = pd.read_csv(\"C:\\\\Users\\\\VIRGIL\\\\anaconda34\\\\Ham10000\\\\HAM10000_metadata.csv\")\n","data_pd.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pUyNO2iuNMqQ"},"outputs":[],"source":["train_dir = os.path.join('C:\\\\Users\\\\VIRGIL\\\\anaconda34\\\\Ham10000\\\\' ,'train_dir')\n","test_dir = os.path.join('C:\\\\Users\\\\VIRGIL\\\\anaconda34\\\\Ham10000\\\\', 'test_dir')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IufYalhmNMqR","outputId":"dfb9ef30-c11e-4b59-9b5f-cdfc5f96f185"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_id</th>\n","      <th>dx</th>\n","      <th>dx_type</th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>localization</th>\n","    </tr>\n","    <tr>\n","      <th>lesion_id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>HAM_0000001</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>HAM_0000002</th>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>HAM_0000005</th>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>HAM_0000006</th>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>HAM_0000009</th>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             image_id  dx  dx_type  age  sex  localization\n","lesion_id                                                 \n","HAM_0000001         1   1        1    1    1             1\n","HAM_0000002         3   3        3    3    3             3\n","HAM_0000005         4   4        4    4    4             4\n","HAM_0000006         3   3        3    3    3             3\n","HAM_0000009         3   3        3    3    3             3"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["df_count = data_pd.groupby('lesion_id').count()\n","df_count.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T1c0CZdoNMqR"},"outputs":[],"source":["df_count = df_count[df_count['dx'] == 1]\n","df_count.reset_index(inplace=True)\n","\n","\n","     \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ajTIjIBtNMqR"},"outputs":[],"source":["def duplicates(x):\n","    unique = set(df_count['lesion_id'])\n","    if x in unique:\n","        return 'no' \n","    else:\n","        return 'duplicates'\n","\n","\n","     \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7qF0vNBjNMqS","outputId":"ea2b1c19-955a-4989-eb51-c763697f0dcc"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>lesion_id</th>\n","      <th>image_id</th>\n","      <th>dx</th>\n","      <th>dx_type</th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>localization</th>\n","      <th>is_duplicate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>HAM_0000118</td>\n","      <td>ISIC_0027419</td>\n","      <td>bkl</td>\n","      <td>histo</td>\n","      <td>80.0</td>\n","      <td>male</td>\n","      <td>scalp</td>\n","      <td>duplicates</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>HAM_0000118</td>\n","      <td>ISIC_0025030</td>\n","      <td>bkl</td>\n","      <td>histo</td>\n","      <td>80.0</td>\n","      <td>male</td>\n","      <td>scalp</td>\n","      <td>duplicates</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>HAM_0002730</td>\n","      <td>ISIC_0026769</td>\n","      <td>bkl</td>\n","      <td>histo</td>\n","      <td>80.0</td>\n","      <td>male</td>\n","      <td>scalp</td>\n","      <td>duplicates</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>HAM_0002730</td>\n","      <td>ISIC_0025661</td>\n","      <td>bkl</td>\n","      <td>histo</td>\n","      <td>80.0</td>\n","      <td>male</td>\n","      <td>scalp</td>\n","      <td>duplicates</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>HAM_0001466</td>\n","      <td>ISIC_0031633</td>\n","      <td>bkl</td>\n","      <td>histo</td>\n","      <td>75.0</td>\n","      <td>male</td>\n","      <td>ear</td>\n","      <td>duplicates</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     lesion_id      image_id   dx dx_type   age   sex localization  \\\n","0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp   \n","1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp   \n","2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp   \n","3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp   \n","4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear   \n","\n","  is_duplicate  \n","0   duplicates  \n","1   duplicates  \n","2   duplicates  \n","3   duplicates  \n","4   duplicates  "]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["\n","data_pd['is_duplicate'] = data_pd['lesion_id'].apply(duplicates)\n","data_pd.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EXRPro5wNMqS"},"outputs":[],"source":["df_count = data_pd[data_pd['is_duplicate'] == 'no']\n","\n","\n","     \n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yks27MIzNMqT"},"outputs":[],"source":["train, test_df = train_test_split(df_count, test_size=0.1, stratify=df_count['dx'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZtYQ3SgINMqT","outputId":"1cb798a1-b968-40b3-a7f2-2a84c465befd"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>lesion_id</th>\n","      <th>image_id</th>\n","      <th>dx</th>\n","      <th>dx_type</th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>localization</th>\n","      <th>is_duplicate</th>\n","      <th>train_test_split</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>HAM_0000118</td>\n","      <td>ISIC_0027419</td>\n","      <td>bkl</td>\n","      <td>histo</td>\n","      <td>80.0</td>\n","      <td>male</td>\n","      <td>scalp</td>\n","      <td>duplicates</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>HAM_0000118</td>\n","      <td>ISIC_0025030</td>\n","      <td>bkl</td>\n","      <td>histo</td>\n","      <td>80.0</td>\n","      <td>male</td>\n","      <td>scalp</td>\n","      <td>duplicates</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>HAM_0002730</td>\n","      <td>ISIC_0026769</td>\n","      <td>bkl</td>\n","      <td>histo</td>\n","      <td>80.0</td>\n","      <td>male</td>\n","      <td>scalp</td>\n","      <td>duplicates</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>HAM_0002730</td>\n","      <td>ISIC_0025661</td>\n","      <td>bkl</td>\n","      <td>histo</td>\n","      <td>80.0</td>\n","      <td>male</td>\n","      <td>scalp</td>\n","      <td>duplicates</td>\n","      <td>train</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>HAM_0001466</td>\n","      <td>ISIC_0031633</td>\n","      <td>bkl</td>\n","      <td>histo</td>\n","      <td>75.0</td>\n","      <td>male</td>\n","      <td>ear</td>\n","      <td>duplicates</td>\n","      <td>train</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     lesion_id      image_id   dx dx_type   age   sex localization  \\\n","0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male        scalp   \n","1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male        scalp   \n","2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male        scalp   \n","3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male        scalp   \n","4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male          ear   \n","\n","  is_duplicate train_test_split  \n","0   duplicates            train  \n","1   duplicates            train  \n","2   duplicates            train  \n","3   duplicates            train  \n","4   duplicates            train  "]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["def identify_trainOrtest(x):\n","    test_data = set(test_df['image_id'])\n","    if str(x) in test_data:\n","        return 'test'\n","    else:\n","        return 'train'\n","\n","#creating train_df\n","data_pd['train_test_split'] = data_pd['image_id'].apply(identify_trainOrtest)\n","train_df = data_pd[data_pd['train_test_split'] == 'train']\n","train_df.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3uYJrMoJNMqT"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yKF0bBwtNMqT","outputId":"58647dfa-e1bb-45fd-c973-ac9d651b40bd"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>lesion_id</th>\n","      <th>image_id</th>\n","      <th>dx</th>\n","      <th>dx_type</th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>localization</th>\n","      <th>is_duplicate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1206</th>\n","      <td>HAM_0001841</td>\n","      <td>ISIC_0025771</td>\n","      <td>df</td>\n","      <td>consensus</td>\n","      <td>35.0</td>\n","      <td>female</td>\n","      <td>lower extremity</td>\n","      <td>no</td>\n","    </tr>\n","    <tr>\n","      <th>2753</th>\n","      <td>HAM_0006465</td>\n","      <td>ISIC_0030813</td>\n","      <td>bcc</td>\n","      <td>histo</td>\n","      <td>65.0</td>\n","      <td>male</td>\n","      <td>back</td>\n","      <td>no</td>\n","    </tr>\n","    <tr>\n","      <th>2772</th>\n","      <td>HAM_0006708</td>\n","      <td>ISIC_0027189</td>\n","      <td>bcc</td>\n","      <td>histo</td>\n","      <td>80.0</td>\n","      <td>female</td>\n","      <td>lower extremity</td>\n","      <td>no</td>\n","    </tr>\n","    <tr>\n","      <th>597</th>\n","      <td>HAM_0000806</td>\n","      <td>ISIC_0024371</td>\n","      <td>bkl</td>\n","      <td>histo</td>\n","      <td>70.0</td>\n","      <td>male</td>\n","      <td>lower extremity</td>\n","      <td>no</td>\n","    </tr>\n","    <tr>\n","      <th>871</th>\n","      <td>HAM_0003010</td>\n","      <td>ISIC_0029924</td>\n","      <td>bkl</td>\n","      <td>consensus</td>\n","      <td>55.0</td>\n","      <td>female</td>\n","      <td>face</td>\n","      <td>no</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        lesion_id      image_id   dx    dx_type   age     sex  \\\n","1206  HAM_0001841  ISIC_0025771   df  consensus  35.0  female   \n","2753  HAM_0006465  ISIC_0030813  bcc      histo  65.0    male   \n","2772  HAM_0006708  ISIC_0027189  bcc      histo  80.0  female   \n","597   HAM_0000806  ISIC_0024371  bkl      histo  70.0    male   \n","871   HAM_0003010  ISIC_0029924  bkl  consensus  55.0  female   \n","\n","         localization is_duplicate  \n","1206  lower extremity           no  \n","2753             back           no  \n","2772  lower extremity           no  \n","597   lower extremity           no  \n","871              face           no  "]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["\n","test_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N20tBCJ-NMqU"},"outputs":[],"source":["train_list = list(train_df['image_id'])\n","test_list = list(test_df['image_id'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c6FakzXSNMqU","outputId":"7eb66cda-468a-40f5-9604-0436b4a8470d"},"outputs":[{"data":{"text/plain":["4496"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["len(test_list)\n","\n","\n","    \n","len(train_list)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PFF4wFivNMqU","outputId":"b9983318-54c1-4cd1-aeb1-7fef3ce41442"},"outputs":[{"data":{"text/plain":["119"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["len(test_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZvQVDj0gNMqU"},"outputs":[],"source":["\n","# Set the image_id as the index in data_pd\n","data_pd.set_index('image_id', inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-DtbY6PaNMqV"},"outputs":[],"source":["os.mkdir(train_dir)\n","os.mkdir(test_dir)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wFonQZiCNMqV"},"outputs":[],"source":["targetnames = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PfZjgYbZNMqV"},"outputs":[],"source":["for i in targetnames:\n","  directory1=train_dir+'/'+i\n","  directory2=test_dir+'/'+i\n","  os.mkdir(directory1)\n","  os.mkdir(directory2)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BWVfqSPfNMqV"},"outputs":[],"source":["\n","for image in train_list:\n","    file_name = image+'.jpg'\n","    label = data_pd.loc[image, 'dx']\n","\n","    # path of source image \n","    source = os.path.join('C:\\\\Users\\\\VIRGIL\\\\anaconda34\\\\Ham10000\\\\All_Images', file_name)\n","\n","    # copying the image from the source to target file\n","    target = os.path.join(train_dir, label, file_name)\n","\n","    shutil.copyfile(source, target)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ET3pI1mpNMqW"},"outputs":[],"source":["for image in test_list:\n","\n","    file_name = image+'.jpg'\n","    label = data_pd.loc[image, 'dx']\n","\n","    # path of source image \n","    source = os.path.join('C:\\\\Users\\\\VIRGIL\\\\anaconda34\\\\Ham10000\\\\All_Images', file_name)\n","\n","    # copying the image from the source to target file\n","    target = os.path.join(test_dir, label, file_name)\n","\n","    shutil.copyfile(source, target)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-tdU1ZGlNMqW","outputId":"f5012dfb-dc08-40d9-ba00-fc8171f14139"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 312 images belonging to 1 classes.\n","Found 496 images belonging to 1 classes.\n","Found 1054 images belonging to 1 classes.\n","Found 111 images belonging to 1 classes.\n","Found 1090 images belonging to 1 classes.\n","Found 1297 images belonging to 1 classes.\n","Found 136 images belonging to 1 classes.\n"]}],"source":["targetnames = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n","\n","# Augmenting images and storing them in temporary directories \n","for img_class in targetnames:\n","\n","    #creating temporary directories\n","    # creating a base directory\n","    Aug_dir = 'Aug_dir'\n","    os.mkdir(Aug_dir)\n","    # creating a subdirectory inside the base directory for images of the same class\n","    img_dir = os.path.join(Aug_dir, 'img_dir')\n","    os.mkdir(img_dir)\n","\n","    img_list = os.listdir('C:\\\\Users\\\\VIRGIL\\\\anaconda34\\\\Ham10000\\\\train_dir\\\\' + img_class)\n","\n","    # Copy images from the class train dir to the img_dir \n","    for file_name in img_list:\n","\n","        # path of source image in training directory\n","        source = os.path.join('C:\\\\Users\\\\VIRGIL\\\\anaconda34\\\\Ham10000\\\\train_dir\\\\' + img_class, file_name)\n","\n","        # creating a target directory to send images \n","        target = os.path.join(img_dir, file_name)\n","\n","        # copying the image from the source to target file\n","        shutil.copyfile(source, target)\n","\n","    # Temporary augumented dataset directory.\n","    source_path = Aug_dir\n","\n","    # Augmented images will be saved to training directory\n","    save_path = 'C:\\\\Users\\\\VIRGIL\\\\anaconda34\\\\Ham10000\\\\train_dir\\\\' + img_class\n","\n","    # Creating Image Data Generator to augment images\n","    datagen = tf.keras.preprocessing.image.ImageDataGenerator(featurewise_center=False, \n","                                    samplewise_center=False, \n","                                    featurewise_std_normalization=False, \n","                                    samplewise_std_normalization=False, \n","                                    zca_whitening=False, \n","                                    zca_epsilon=1e-06, \n","                                    rotation_range=15, \n","                                    width_shift_range=0.15, \n","                                    height_shift_range=0.15, \n","                                    brightness_range=None, \n","                                    shear_range=0.1, \n","                                    zoom_range=0.1, \n","                                    channel_shift_range=0.1, \n","                                    fill_mode='nearest', \n","                                    cval=0.0, \n","                                    horizontal_flip=True, \n","                                    vertical_flip=True, \n","                                    rescale=None, \n","                                    preprocessing_function=None, \n","                                    data_format='channels_last', \n","                                    validation_split=0.0, \n","                                    interpolation_order=1, \n","                                    dtype='float32')\n","\n","    batch_size = 50\n","\n","    Aug_datagen = datagen.flow_from_directory(source_path,save_to_dir=save_path,save_format='jpg',target_size=(450, 600),batch_size=batch_size)\n","\n","    # Generate the augmented images\n","    Aug_images = 1450\n","\n","    num_files = len(os.listdir(img_dir))\n","    num_batches = int(np.ceil((Aug_images - num_files) / batch_size))\n","\n","    # creating 8000 augmented images per class\n","    for i in range(0, num_batches):\n","        images, labels = next(Aug_datagen)\n","\n","    # delete temporary directory \n","    shutil.rmtree('Aug_dir')"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.13 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"2917a190c2942419e6bc3e4aec168ab918ffcf7cc870d6cd5301b8ee1a607baa"}},"colab":{"provenance":[{"file_id":"1iyGqVpPTdnXWrBbEBakztnnJ5Ze222gF","timestamp":1672138346767}]}},"nbformat":4,"nbformat_minor":0}